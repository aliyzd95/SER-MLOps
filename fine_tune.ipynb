{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-06-06T11:49:51.357532Z",
     "start_time": "2025-06-06T11:49:51.218707Z"
    }
   },
   "source": [
    "gpu_info = !nvidia-smi\n",
    "gpu_info = '\\n'.join(gpu_info)\n",
    "if gpu_info.find('failed') >= 0:\n",
    "    print('Not connected to a GPU')\n",
    "else:\n",
    "    print(gpu_info)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fri Jun  6 15:19:51 2025       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 576.28                 Driver Version: 576.28         CUDA Version: 12.9     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                  Driver-Model | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  NVIDIA GeForce RTX 3070 ...  WDDM  |   00000000:01:00.0  On |                  N/A |\n",
      "| N/A   54C    P8             17W /   88W |     426MiB /   8192MiB |      2%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|    0   N/A  N/A            4332    C+G   ...ntrolPanel\\SystemSettings.exe      N/A      |\n",
      "|    0   N/A  N/A            7028    C+G   ...4__8wekyb3d8bbwe\\Video.UI.exe      N/A      |\n",
      "|    0   N/A  N/A            7228    C+G   ...Chrome\\Application\\chrome.exe      N/A      |\n",
      "|    0   N/A  N/A            9536    C+G   ...2025.1\\jbr\\bin\\cef_server.exe      N/A      |\n",
      "|    0   N/A  N/A           10056    C+G   C:\\Windows\\explorer.exe               N/A      |\n",
      "|    0   N/A  N/A           10304    C+G   ...IA app\\CEF\\NVIDIA Overlay.exe      N/A      |\n",
      "|    0   N/A  N/A           11604    C+G   ...IA app\\CEF\\NVIDIA Overlay.exe      N/A      |\n",
      "|    0   N/A  N/A           13280    C+G   ...h_cw5n1h2txyewy\\SearchApp.exe      N/A      |\n",
      "|    0   N/A  N/A           16132    C+G   ...8bbwe\\PhoneExperienceHost.exe      N/A      |\n",
      "|    0   N/A  N/A           16272    C+G   ...App_cw5n1h2txyewy\\LockApp.exe      N/A      |\n",
      "|    0   N/A  N/A           16772    C+G   ...t\\Edge\\Application\\msedge.exe      N/A      |\n",
      "|    0   N/A  N/A           17660    C+G   ...5n1h2txyewy\\TextInputHost.exe      N/A      |\n",
      "|    0   N/A  N/A           17960    C+G   ...xyewy\\ShellExperienceHost.exe      N/A      |\n",
      "|    0   N/A  N/A           19408    C+G   ...ekyb3d8bbwe\\CalculatorApp.exe      N/A      |\n",
      "+-----------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-06T11:49:57.685357Z",
     "start_time": "2025-06-06T11:49:55.073397Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from datasets import load_dataset, Audio, concatenate_datasets, DatasetDict\n",
    "import os\n",
    "\n",
    "clips_dir = \"D:/projects/SPEECH/cv-corpus-21.0-2025-03-14/fa/clips\"\n",
    "data_files = {\n",
    "    \"train\": \"D:/projects/SPEECH/cv-corpus-21.0-2025-03-14/fa/train.csv\",\n",
    "    \"test\": \"D:/projects/SPEECH/cv-corpus-21.0-2025-03-14/fa/test.csv\",\n",
    "    \"validation\": \"D:/projects/SPEECH/cv-corpus-21.0-2025-03-14/fa/dev.csv\"\n",
    "}\n",
    "\n",
    "dataset = load_dataset(\"csv\", data_files=data_files)\n",
    "\n",
    "\n",
    "def add_full_path(example):\n",
    "    example[\"path\"] = os.path.join(clips_dir, example[\"path\"])\n",
    "    return example\n",
    "\n",
    "\n",
    "dataset = dataset.map(add_full_path)\n",
    "\n",
    "dataset = dataset.cast_column(\"path\", Audio(sampling_rate=16000))\n",
    "\n",
    "for split in dataset:\n",
    "    dataset[split] = dataset[split].rename_columns({\n",
    "        \"path\": \"audio\",\n",
    "        \"sentence\": \"text\"\n",
    "    })\n",
    "\n",
    "for split in dataset:\n",
    "    dataset[split] = dataset[split].remove_columns(\n",
    "        [col for col in dataset[split].column_names if col not in [\"audio\", \"text\"]]\n",
    "    )\n",
    "\n",
    "train_dataset = concatenate_datasets([dataset[\"train\"], dataset[\"validation\"]])\n",
    "test_dataset = dataset[\"test\"]\n",
    "\n",
    "common_voice = DatasetDict()\n",
    "\n",
    "common_voice[\"train\"] = train_dataset\n",
    "common_voice[\"test\"] = test_dataset\n",
    "\n",
    "print(common_voice)"
   ],
   "id": "b146a8f707b5ae1b",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\anaconda3\\envs\\torch_gpu\\Lib\\site-packages\\requests\\__init__.py:86: RequestsDependencyWarning: Unable to find acceptable character detection dependency (chardet or charset_normalizer).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['audio', 'text'],\n",
      "        num_rows: 40397\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['audio', 'text'],\n",
      "        num_rows: 10668\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-06T11:50:03.690859Z",
     "start_time": "2025-06-06T11:49:58.315566Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from transformers import WhisperFeatureExtractor\n",
    "\n",
    "feature_extractor = WhisperFeatureExtractor.from_pretrained(\"openai/whisper-small\")"
   ],
   "id": "10a742d137450674",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-06T11:50:04.270158Z",
     "start_time": "2025-06-06T11:50:03.699985Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from transformers import WhisperTokenizer\n",
    "\n",
    "tokenizer = WhisperTokenizer.from_pretrained(\"openai/whisper-small\", language=\"Persian\", task=\"transcribe\")"
   ],
   "id": "5288a32ad73cbfd5",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-06T11:50:07.997323Z",
     "start_time": "2025-06-06T11:50:04.281312Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from transformers import WhisperProcessor\n",
    "\n",
    "processor = WhisperProcessor.from_pretrained(\"openai/whisper-small\", language=\"Persian\", task=\"transcribe\")"
   ],
   "id": "39d432c930dbaccb",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-06T11:50:10.195359Z",
     "start_time": "2025-06-06T11:50:08.153753Z"
    }
   },
   "cell_type": "code",
   "source": "print(common_voice[\"train\"][0])",
   "id": "d6cae39adbae4110",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'audio': {'path': 'D:/projects/SPEECH/cv-corpus-21.0-2025-03-14/fa/clips\\\\common_voice_fa_24473237.mp3', 'array': array([-3.49245965e-10,  1.39698386e-09, -3.49245965e-10, ...,\n",
      "        2.97557563e-07,  1.62050128e-07,  3.09199095e-07]), 'sampling_rate': 16000}, 'text': 'خشم کسی را دامن زدن'}\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-06T11:50:10.371472Z",
     "start_time": "2025-06-06T11:50:10.363433Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from datasets import Audio\n",
    "\n",
    "common_voice = common_voice.cast_column(\"audio\", Audio(sampling_rate=16000))"
   ],
   "id": "eafc4af75691072d",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-06T11:50:10.554168Z",
     "start_time": "2025-06-06T11:50:10.549806Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def prepare_dataset(batch):\n",
    "    # load and resample audio data from 48 to 16kHz\n",
    "    audio = batch[\"audio\"]\n",
    "\n",
    "    # compute log-Mel input features from input audio array\n",
    "    batch[\"input_features\"] = feature_extractor(audio[\"array\"], sampling_rate=audio[\"sampling_rate\"]).input_features[0]\n",
    "\n",
    "    # encode target text to label ids\n",
    "    batch[\"labels\"] = tokenizer(batch[\"text\"]).input_ids\n",
    "    return batch"
   ],
   "id": "c7e7ae302047e0c1",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-06T11:50:12.804246Z",
     "start_time": "2025-06-06T11:50:10.722083Z"
    }
   },
   "cell_type": "code",
   "source": "common_voice = common_voice.map(prepare_dataset, remove_columns=common_voice.column_names[\"train\"])",
   "id": "89df0d2ad0baeb65",
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-06T11:50:14.544694Z",
     "start_time": "2025-06-06T11:50:12.965973Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from transformers import WhisperForConditionalGeneration\n",
    "\n",
    "model = WhisperForConditionalGeneration.from_pretrained(\"openai/whisper-small\")"
   ],
   "id": "b7b2eeab8377ee0c",
   "outputs": [],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-06T11:50:14.697388Z",
     "start_time": "2025-06-06T11:50:14.694027Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model.generation_config.language = \"Persian\"\n",
    "model.generation_config.task = \"transcribe\"\n",
    "\n",
    "model.generation_config.forced_decoder_ids = None"
   ],
   "id": "9d4e77a7edf1d783",
   "outputs": [],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-06T11:50:14.865852Z",
     "start_time": "2025-06-06T11:50:14.858321Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "\n",
    "from dataclasses import dataclass\n",
    "from typing import Any, Dict, List, Union\n",
    "\n",
    "@dataclass\n",
    "class DataCollatorSpeechSeq2SeqWithPadding:\n",
    "    processor: Any\n",
    "    decoder_start_token_id: int\n",
    "\n",
    "    def __call__(self, features: List[Dict[str, Union[List[int], torch.Tensor]]]) -> Dict[str, torch.Tensor]:\n",
    "        # split inputs and labels since they have to be of different lengths and need different padding methods\n",
    "        # first treat the audio inputs by simply returning torch tensors\n",
    "        input_features = [{\"input_features\": feature[\"input_features\"]} for feature in features]\n",
    "        batch = self.processor.feature_extractor.pad(input_features, return_tensors=\"pt\")\n",
    "\n",
    "        # get the tokenized label sequences\n",
    "        label_features = [{\"input_ids\": feature[\"labels\"]} for feature in features]\n",
    "        # pad the labels to max length\n",
    "        labels_batch = self.processor.tokenizer.pad(label_features, return_tensors=\"pt\")\n",
    "\n",
    "        # replace padding with -100 to ignore loss correctly\n",
    "        labels = labels_batch[\"input_ids\"].masked_fill(labels_batch.attention_mask.ne(1), -100)\n",
    "\n",
    "        # if bos token is appended in previous tokenization step,\n",
    "        # cut bos token here as it's append later anyways\n",
    "        if (labels[:, 0] == self.decoder_start_token_id).all().cpu().item():\n",
    "            labels = labels[:, 1:]\n",
    "\n",
    "        batch[\"labels\"] = labels\n",
    "\n",
    "        return batch"
   ],
   "id": "3315511b12140de3",
   "outputs": [],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-06T11:50:15.035237Z",
     "start_time": "2025-06-06T11:50:15.031309Z"
    }
   },
   "cell_type": "code",
   "source": [
    "data_collator = DataCollatorSpeechSeq2SeqWithPadding(\n",
    "    processor=processor,\n",
    "    decoder_start_token_id=model.config.decoder_start_token_id,\n",
    ")"
   ],
   "id": "e9672ce284b6c189",
   "outputs": [],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-06T11:50:16.900348Z",
     "start_time": "2025-06-06T11:50:15.347697Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import evaluate\n",
    "\n",
    "metric = evaluate.load(\"wer\")"
   ],
   "id": "af86b4db318e624",
   "outputs": [],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-06T11:50:17.210607Z",
     "start_time": "2025-06-06T11:50:17.206036Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def compute_metrics(pred):\n",
    "    pred_ids = pred.predictions\n",
    "    label_ids = pred.label_ids\n",
    "\n",
    "    # replace -100 with the pad_token_id\n",
    "    label_ids[label_ids == -100] = tokenizer.pad_token_id\n",
    "\n",
    "    # we do not want to group tokens when computing the metrics\n",
    "    pred_str = tokenizer.batch_decode(pred_ids, skip_special_tokens=True)\n",
    "    label_str = tokenizer.batch_decode(label_ids, skip_special_tokens=True)\n",
    "\n",
    "    wer = 100 * metric.compute(predictions=pred_str, references=label_str)\n",
    "\n",
    "    return {\"wer\": wer}"
   ],
   "id": "56141b7d4f4fd7eb",
   "outputs": [],
   "execution_count": 15
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-06T11:50:20.107163Z",
     "start_time": "2025-06-06T11:50:20.101696Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "from transformers import WhisperTokenizer\n",
    "\n",
    "tokenizer = WhisperTokenizer.from_pretrained(\"openai/whisper-small\")\n",
    "\n",
    "lengths = [len(tokenizer(sample[\"text\"]).input_ids) for sample in dataset[\"test\"]]\n",
    "max_length = max(lengths)\n",
    "percentile_95 = int(np.percentile(lengths, 95))\n",
    "\n",
    "print(\"Max length:\", max_length)\n",
    "print(\"95th percentile:\", percentile_95)"
   ],
   "id": "2c65a281ef22fcd4",
   "outputs": [],
   "execution_count": 16
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-06T11:50:36.282571Z",
     "start_time": "2025-06-06T11:50:36.175290Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from transformers import Seq2SeqTrainingArguments\n",
    "\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=\"./whisper-small-persian-v1\",  # change to a repo name of your choice\n",
    "    per_device_train_batch_size=8,\n",
    "    gradient_accumulation_steps=2,  # increase by 2x for every 2x decrease in batch size\n",
    "    # group_by_length=True,\n",
    "    learning_rate=4e-5,\n",
    "    warmup_steps=500,\n",
    "    # max_steps=4000,\n",
    "    num_train_epochs=3,\n",
    "    gradient_checkpointing=True,\n",
    "    fp16=True,\n",
    "    eval_strategy=\"steps\",\n",
    "    # eval_strategy=\"no\",\n",
    "    save_strategy=\"steps\",\n",
    "    save_total_limit=1,\n",
    "    per_device_eval_batch_size=8,\n",
    "    predict_with_generate=True,\n",
    "    generation_max_length=48,\n",
    "    save_steps=1000,\n",
    "    eval_steps=1000,\n",
    "    logging_steps=50,\n",
    "    logging_dir=\"./logs\",\n",
    "    report_to=[\"tensorboard\"],\n",
    "    # load_best_model_at_end=False,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"wer\",\n",
    "    greater_is_better=False,\n",
    "    push_to_hub=False,\n",
    "    resume_from_checkpoint='last-checkpoint',\n",
    ")"
   ],
   "id": "7c85834dbb31ad11",
   "outputs": [],
   "execution_count": 17
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-06T11:51:10.738745Z",
     "start_time": "2025-06-06T11:51:08.921877Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from transformers import Seq2SeqTrainer, EarlyStoppingCallback\n",
    "\n",
    "trainer = Seq2SeqTrainer(\n",
    "    args=training_args,\n",
    "    model=model,\n",
    "    train_dataset=common_voice[\"train\"],\n",
    "    eval_dataset=common_voice[\"test\"],\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    "    tokenizer=processor.feature_extractor,\n",
    "    # callbacks=[EarlyStoppingCallback(early_stopping_patience=3)]\n",
    ")"
   ],
   "id": "bb6156ea7ed6e2d4",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\A L I\\AppData\\Local\\Temp\\ipykernel_15804\\3731876365.py:3: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Seq2SeqTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Seq2SeqTrainer(\n"
     ]
    }
   ],
   "execution_count": 18
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-06T02:29:18.107810Z",
     "start_time": "2025-06-06T02:29:17.845601Z"
    }
   },
   "cell_type": "code",
   "source": "processor.save_pretrained(training_args.output_dir)",
   "id": "363a6a8bb2b5f28b",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 19
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "trainer.train()\n",
    "# trainer.train(resume_from_checkpoint=True)"
   ],
   "id": "6a19d3aaf57a44ea",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "# tensorboard --logdir=./logs --port=6006\n",
   "id": "8492d98fee89c094"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
